{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2ee2f2-9d43-4fdf-bec1-8b89ac95dfe9",
   "metadata": {},
   "source": [
    "# ML Training Pipeline\n",
    "\n",
    "Author: Marco Pellegrino<br>\n",
    "Year: 2024\n",
    "\n",
    "This overall project aims to build a simple model to predict the probability of loan default based on loan application data. This information helps assess business risk and improve loan approval decisions.\n",
    "\n",
    "In this notebook, a Decision Tree model is tuned, trained, and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac323dc3-3303-4475-a7c6-7c768d1448a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import roc_curve, auc, log_loss, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree, export_text\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "# Import paths\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a89f3-c083-46e7-8300-b5e6f2c2adf5",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f722bd-e4ad-4048-8f5f-de9c70bb32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = pd.read_csv(PATH_DATA_PREPROCESSED+'loan_application_data-preprocessed.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e7d0d-159f-420f-a68b-5683e5c64aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b88274-dbb3-4108-a06c-50857a5cf939",
   "metadata": {},
   "source": [
    "# Identify features requiring data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fa1364-d2f2-4bd5-a4dc-79dbdc30f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_na_info(df):\n",
    "    \"\"\" Return total and percentages of NA in a data frame for each feature\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'NaN Count': df.isna().sum(),\n",
    "        'NaN Percentage (%)': round((df.isna().sum() / len(df)) * 100, 2)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ab741e-1afb-482b-a74e-f8721276fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many null values there are\n",
    "display(get_na_info(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77419268-7ccf-45dc-8362-cbd03ea6da1d",
   "metadata": {},
   "source": [
    "- The `uc_risk_class` feature contains a substantial amount of missing data, necessitating its removal. Due to the insufficient data, imputing values using median imputation or machine learning techniques is not feasible. Moreover, as UC Risk Class is typically derived from existing data in the data frame, eliminating `uc_risk_class` should not result in significant information loss. Consequently, reducing the feature quantity is expected to decrease model complexity, leading to improved performance. The use of `uc_risk_class` can be reconsidered once more updated client cases are collected in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022b84a-41db-4266-994f-6c76685acc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['uc_risk_class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f423690-281c-4c73-96a7-1655b695e428",
   "metadata": {},
   "source": [
    "- `company_rating`, `person_scoring`, and `incorporation_days` have very few NA.\n",
    "It is possible to remove such NA entries in those features because such a reduction will not have a big impact both overall and on their distributions. As a benefit, it will increase the overall data quality. This can be done now because missing values removal does not lead to data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c75af2-b1d4-4b73-af0d-03a166064fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['company_rating', 'incorporation_days', 'person_scoring']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663f3ad-e53d-4ce8-afc8-de33296deff4",
   "metadata": {},
   "source": [
    "- `net_turnover` has several missing data points, and imputation can be employed for reconstruction. Various techniques, including mode imputation, K-Nearest Neighbors (KNN) imputation, and Random Forest imputation, can be considered. For simplicity and lack of time to tune KNN or ML models, median imputation is used. Median imputation is chosen over mean imputation, considering that the features `net_turnover` exhibits a high degree of skewness. Mean imputation is suitable for approximately normally distributed and non-skewed data.\n",
    "It is advisable to avoid the following imputation techniques for this feature:\n",
    "  - Forward Fill or Backward Fill: Data order is not significant.\n",
    "  - Linear Regression Imputation: No linear relationship is observed (refer to the plot at the end of the notebook).\n",
    "  - Deep Learning Imputation: Given the sensitive financial nature of the feature, a transparent method is preferred.\n",
    "    \n",
    "Imputation must be done during the training phase to avoid data leakage to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d34107-702c-4207-b9c9-730419fdfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again how many null values there are\n",
    "display(get_na_info(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f377b38-7b80-48dd-8ab3-3abef6fa7b24",
   "metadata": {},
   "source": [
    "# Split data between features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a21cf8-c526-45c2-84b8-ff0bbf5caead",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['default'], axis=1)\n",
    "y = df['default']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdda4e9d-12d3-4966-bba9-d891a06aeb72",
   "metadata": {},
   "source": [
    "# Split data between Training and Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e13b8-2ea8-49e7-9823-8b885a5f7b10",
   "metadata": {},
   "source": [
    "Split making sure that the random split has the same original target distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df58e9eb-e262-4296-8102-a5174a0ffc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e439045-cbf4-4d40-a455-7725d6bb222c",
   "metadata": {},
   "source": [
    "# Imbalanced target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9680e4d-19b3-43b6-8312-85645088e457",
   "metadata": {},
   "source": [
    "Firstly, let's recall the imbalanced target, this time on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149fb17-9af8-48d9-b7eb-52b1b4c5c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "ax = sns.countplot(x=y_train, palette=\"Set3\", hue=y_train, legend=False)\n",
    "sns.set(font_scale=1.5)\n",
    "ax.set_xlabel('Loan Default')\n",
    "ax.set_ylabel('Frequency')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(10, 5)\n",
    "\n",
    "# Adding percentage labels on each bar\n",
    "total = len(y_train)\n",
    "for p in ax.patches:\n",
    "    percentage = '{:.1f}%'.format(100 * p.get_height() / total)\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height()\n",
    "    ax.annotate(percentage, (x, y), ha='center', va='bottom')\n",
    "\n",
    "plt.title('RAW Distribution of Loan Default')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bded535-f6cc-423e-a823-6808b5607105",
   "metadata": {},
   "source": [
    "There are 3 possible solutions:\n",
    "- Under-sampling: give more weight to the positive class, by removing negative observations to reach an equal target balance. However, it sacrificies a lot observations: training set size would be around 40000 records\n",
    "- Over-sampling: give more weight to the positive class, by duplicating positive observations to reach an equal target balance. However, it might lead to overfitting because of repeated data points. Also, having more data points to train lead to longer training time\n",
    "- Class weight: modifies the model loss function by giving more penalty to the positive class which has more weight because under-represented\n",
    "\n",
    "Class weight is used because:\n",
    "- It does not affect the dataset's size\n",
    "- It is less computationally expensive (over-sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63151cd0-12be-4ee0-898d-068132544e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = compute_sample_weight(\n",
    "    class_weight='balanced',\n",
    "    y=y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a2616f-9ad6-4299-aa0f-881495d5c6d9",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce9597-9827-4b6e-bd3f-df03da5ef16e",
   "metadata": {},
   "source": [
    "Create a Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ffba5a-2aff-4c5a-8177-02ec0b4f1012",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae4ffef-d0d2-48d8-a4aa-87bef758c10c",
   "metadata": {},
   "source": [
    "Define hyperparameters to tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad2ba4-7394-488d-bfe7-5e1866876d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75472521-43fd-4f93-97d6-3036fd16d821",
   "metadata": {},
   "source": [
    "To avoid data leakage, median imputation for `net_turnover` is performed in each fold of the cross-validation tuning search. To do so, a pipeline is created, where first the imputation is performed and then the model is trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4d774-ddcf-4430-a1b7-5557115b4072",
   "metadata": {},
   "source": [
    "Add prefix to parameters so that the pipeline knows that the parameters apply to the model and not the imputer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065a4a6c-0ed8-4ec5-90a8-4ef9a5f230ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_prefix_to_params(param_dict, prefix, action='add'):\n",
    "    \"\"\"\n",
    "    Add or remove a prefix to/from each parameter in the parameter dictionary.\n",
    "\n",
    "    Args:\n",
    "    - param_dict (dict): A dictionary containing parameter names as keys and their respective values.\n",
    "    - prefix (str): The prefix to be added to or removed from each parameter name.\n",
    "    - action (str): 'add' to add the prefix, 'remove' to remove it. Default is 'add'.\n",
    "\n",
    "    Returns:\n",
    "    - dict: A new dictionary with the prefix added to or removed from each parameter name.\n",
    "    \"\"\"\n",
    "    if action == 'add':\n",
    "        return {f\"{prefix}{param}\": values for param, values in param_dict.items()}\n",
    "    elif action == 'remove':\n",
    "        prefix_length = len(prefix)\n",
    "        return {param[prefix_length:]: values for param, values in param_dict.items() if param.startswith(prefix)}\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action. Please specify 'add' or 'remove'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1006f27-e1ab-4349-b91f-1dac38fef8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = modify_prefix_to_params(param_grid, 'model__', action='add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4edad78-3f07-4560-8c5d-3975bedbcf58",
   "metadata": {},
   "source": [
    "Define mode imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201615d4-d530-406d-9498-c29c95fa07f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_imputer = make_column_transformer(\n",
    "  (SimpleImputer(strategy='median'), ['net_turnover']),\n",
    "  remainder='passthrough',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d08d065-02ca-479a-b37a-276788fe26c8",
   "metadata": {},
   "source": [
    "Build 2-steps pipeline with imputer and model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9973b-7014-4cd0-a66a-eb9948ab6099",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('median_imputer', median_imputer), \n",
    "    ('model', model)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d545f-cafc-41d5-9b2c-759fa7d85f33",
   "metadata": {},
   "source": [
    "Define hypeparameter tuning search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62863df0-6ee0-44e9-88c9-606adbe9584e",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = param_grid,\n",
    "    cv = 10,\n",
    "    scoring='neg_log_loss',\n",
    "    n_iter = 20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb14110-a75b-4079-9524-93d4910cdd83",
   "metadata": {},
   "source": [
    "#### Execute hypeparameter tuning search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7804653b-8724-40d8-83d5-000a8fc3189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add the sample_weight for the model so that each fold gives equal importance to positive and negative observations, avoiding imbalanced classes\n",
    "search_results.fit(X_train, y_train, **{'model__sample_weight': sample_weight})\n",
    "end_time = time.time()\n",
    "\n",
    "minutes, seconds = divmod(end_time-start_time, 60)\n",
    "print(f'Time for hyperparameter tuning: {int(minutes):02d}:{seconds:.2f} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232fc6a4-4b30-4d28-971f-5592f06461a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = search_results.best_params_\n",
    "\n",
    "print('Best parameter set:')\n",
    "param_grid_print = modify_prefix_to_params(best_params, 'model__', action='remove')\n",
    "for param, value in param_grid_print.items():\n",
    "    print(f'{param}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0702a92-1cac-4200-8c36-78185bfa9ca9",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41c3175-14b9-4ec3-ba75-98ebdf660de7",
   "metadata": {},
   "source": [
    "Train on the whole training set with the found best performing parameter set. By reusing the pipeline, the imputation is done again too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d0d283-3d30-42e6-a646-3b33bb6e760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pipeline = pipeline.set_params(**best_params).fit(X_train, y_train, **{'model__sample_weight': sample_weight})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ef9a6-4558-4e74-abfe-569e6876624e",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e93aef-0e70-4bd7-9ca8-1af2ca9afdbc",
   "metadata": {},
   "source": [
    "Make predictions on unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad6e81e-ea45-4479-9f46-cc4ca3cbf3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability prediction on positive class\n",
    "y_pred_prob = final_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Extract predicted class with threshold\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_prob > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9800cae0-8608-41cd-8d9b-17e5b023b1af",
   "metadata": {},
   "source": [
    "#### ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb504cff-f520-4183-8782-146784328b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and AUC score\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(PATH_PLOTS_AUC):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(PATH_PLOTS_AUC)\n",
    "plt.savefig(PATH_PLOTS_AUC+\"auc_roc_curve-DT.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc8826-1dbb-480d-baf0-2a78b3bda7b9",
   "metadata": {},
   "source": [
    "#### Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5a9d8f-0e1f-4ef6-857d-e4da42baa1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "logloss = log_loss(y_test, y_pred_prob)\n",
    "\n",
    "print(\"Logloss: %.2f\" % logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6656c8-5d81-47f3-b27c-8eafc6bade04",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "f1_target_pos = f1_score(y_test, y_pred, average='binary', pos_label=1)\n",
    "f1_target_neg = f1_score(y_test, y_pred, average='binary', pos_label=0)\n",
    "\n",
    "print(\"Weighted F1 score: %.2f\" % f1_weighted)\n",
    "print(\"F1 score positive class: %.2f\" % f1_target_pos)\n",
    "print(\"F1 negative class: %.2f\" % f1_target_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0841e2e-c84b-435a-93c2-2832bdb36e2d",
   "metadata": {},
   "source": [
    "#### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefda213-a890-4abf-88a1-6cb2d10b8563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the directory exists\n",
    "if not os.path.exists(PATH_RESULTS):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(PATH_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d036cd6-d2fe-4966-a9e9-927b08278940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame({'Model': ['Decision Tree'],\n",
    "                           'ROC-AUC': [roc_auc],\n",
    "                           'LogLoss': [logloss],\n",
    "                           'F1 Weighted-averaged': [f1_weighted],\n",
    "                           'F1 Default=1': [f1_target_pos],\n",
    "                           'F1 Default=0': [f1_target_neg]})\n",
    "\n",
    "# Save\n",
    "df_results.to_csv(PATH_RESULTS+'all/evaluation-DT.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f81ec-f4d0-4258-a706-5aac9b9fe393",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_df = pd.DataFrame({'Decision Tree': fpr})\n",
    "tpr_df = pd.DataFrame({'Decision Tree': tpr})\n",
    "\n",
    "# Save\n",
    "fpr_df.to_csv(PATH_RESULTS+'fpr/evaluation_fpr-DT.csv', header=True, index=False)\n",
    "tpr_df.to_csv(PATH_RESULTS+'tpr/evaluation_tpr-DT.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724839-ea31-44d9-9f9e-6124c57f7775",
   "metadata": {},
   "source": [
    "# Save model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfcc7c7-57d8-4a74-8753-2eedd8153940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the directory for the mode dump exists\n",
    "if not os.path.exists(ML_MODEL_PATH):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(ML_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288020cd-ce2e-4325-8fc2-0878015a35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model from pipeline\n",
    "final_model = pipeline.named_steps['model']\n",
    "\n",
    "# Save\n",
    "joblib.dump(final_model, ML_MODEL_PATH+'DT.pkl')\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d300a4c6-ce34-48dd-a1d0-d7365de20e10",
   "metadata": {},
   "source": [
    "# Extra: Model Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25284260-2823-46a3-a929-29068067fd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance\n",
    "feature_importance = final_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "})\n",
    "\n",
    "# Sort features by importance\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df, palette='viridis', hue='Feature', dodge=False)\n",
    "plt.title('Decision Tree - Feature Importance')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Check if the directory for the plot exists\n",
    "if not os.path.exists(PATH_PLOTS_FEATURE_IMPORTANCE):\n",
    "    # If it doesn't exist, create the directory\n",
    "    os.makedirs(PATH_PLOTS_FEATURE_IMPORTANCE)\n",
    "\n",
    "plt.savefig(PATH_PLOTS_FEATURE_IMPORTANCE+\"feature_importance-DT.png\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67b621a-0ec0-4d29-a305-68996b6539c4",
   "metadata": {},
   "source": [
    "# Extra: Inspect Tree Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdaae2-7581-4cab-9cba-812c3e6edf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the decision tree\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(final_model, filled=True, feature_names=X.columns, class_names=['0', '1'])\n",
    "plt.savefig('plots/models/rules_decision_tree.png', dpi=300) # in high resolutions to see better\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ec01bc-13fb-4208-85b4-8d3b0ee7ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text version\n",
    "tree_rules = export_text(final_model, feature_names=X.columns, show_weights=True)\n",
    "print(tree_rules)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (marcoenv)",
   "language": "python",
   "name": "marcoenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
